"""
Python code extracted from Speaker_Diarization.ipynb
Generated by Project Publisher

Original notebook: 28 cells
Code cells: 25
"""

# Imports
from groq import Groq
from gtts import gTTS
from pydub import AudioSegment
from pytube import YouTube
from torchaudio.transforms import Resample
from transformers import Wav2Vec2Processor
from transformers.models.wav2vec2.modeling_wav2vec2 import (
import assemblyai as aai
import numpy as np
import os
import torch
import torch.nn as nn
import torchaudio
import whisper

# Main code
!pip install -q groq
!pip install -q git+https://github.com/openai/whisper.git
!pip install -q gtts
!pip install -q pytube
# Download audio from YouTube video
def download_audio_from_youtube(youtube_url, output_file):
    yt = YouTube(youtube_url)
    audio_stream = yt.streams.filter(only_audio=True).first()
    audio_stream.download(filename=output_file)
    return output_file
# Transcribe audio using Whisper
def transcribe_audio_with_whisper(audio_file):
    model = whisper.load_model("base")
    result = model.transcribe(audio_file)
    return result["text"]
client = Groq(api_key = "YOUR_API_KEY_HERE")
# Function to get model completion
def get_completion(prompt, model="llama3-70b-8192"):
    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0
    )
    return response.choices[0].message.content
# Analyze text and create Q&A
def create_qa_from_text(text, model="llama3-70b-8192"):
    prompt = f"""
Create an interesting, informative, and interactive set of 5 textual questions and answers (Q&A) from the content which are relevant to the topic or topics in the text. Generate them as Question 1 and Answer 1 and so on.
Text: {text}
"""
    completion = get_completion(prompt, model)
    return completion
# Convert Q&A to speech using gTTS
def convert_qa_to_speech(qa_text, output_file):
    tts = gTTS(text=qa_text, lang='en')
    tts.save(output_file)
# Main execution
youtube_url = "https://www.youtube.com/watch?v=yX5EJf4R77s"
audio_file = "audio.mp3"
download_audio_from_youtube(youtube_url, audio_file)
print(f"Downloaded audio to {audio_file}")
transcript = transcribe_audio_with_whisper(audio_file)
print("Transcription completed")
# Save the transcript to a text file
with open("transcript.txt", "w") as file:
    file.write(transcript)
print("Transcript saved as transcript.txt")
# Create Q&A from the transcript
qa_text = create_qa_from_text(transcript)
# Save the Q&A to a text file
qa_file = "q_and_a.txt"
with open(qa_file, "w") as file:
    file.write(qa_text)
print("Q&A saved as q_and_a.txt")
# Read the Q&A text from file and remove asterisks
with open(qa_file, "r") as file:
    cleaned_qa_text = file.read().replace('*', '')
# Save the cleaned Q&A back to the file
with open(qa_file, "w") as file:
    file.write(cleaned_qa_text)
print("Q&A file cleaned from asterisks")
# Convert Q&A to speech using gTTS
output_audio_file = "q_and_a_audio.mp3"
convert_qa_to_speech(cleaned_qa_text, output_audio_file)
print(f"Q&A audio saved as {output_audio_file}")
!pip install -U assemblyai
!pip install -q pytube
!pip install pydub -q
!pip install torch torchaudio transformers
    Wav2Vec2Model,
    Wav2Vec2PreTrainedModel,
)
# Download audio from YouTube
def download_audio_from_youtube(youtube_url, output_file):
    yt = YouTube(youtube_url)
    audio_stream = yt.streams.filter(only_audio=True).first()
    audio_stream.download(filename=output_file)
link = "https://www.youtube.com/live/8adsVq7Sarw?si=bn98dnLI_fPXwiWq"
download_audio_from_youtube(link, 'cricket_audio.mp3')
# Transcribe audio and get speaker segments using AssemblyAI
aai.settings.api_key = "YOUR_API_KEY_HERE"
FILE_URL = 'cricket_audio.mp3'
config = aai.TranscriptionConfig(speaker_labels=True)
transcriber = aai.Transcriber()
transcript = transcriber.transcribe(FILE_URL, config=config)
speaker_segments = transcript.utterances
# Define the model for age and gender classification
class ModelHead(nn.Module):
    def __init__(self, config, num_labels):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.dropout = nn.Dropout(config.final_dropout)
        self.out_proj = nn.Linear(config.hidden_size, num_labels)
    def forward(self, features, **kwargs):
        x = features
        x = self.dropout(x)
        x = self.dense(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x
class AgeGenderModel(Wav2Vec2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.config = config
        self.wav2vec2 = Wav2Vec2Model(config)
        self.age = ModelHead(config, 1)
        self.gender = ModelHead(config, 3)
        self.init_weights()
    def forward(self, input_values):
        outputs = self.wav2vec2(input_values)
        hidden_states = outputs[0]
        hidden_states = torch.mean(hidden_states, dim=1)
        logits_age = self.age(hidden_states)
        logits_gender = torch.softmax(self.gender(hidden_states), dim=1)
        return hidden_states, logits_age, logits_gender
device = 'cpu'
model_name = 'audeering/wav2vec2-large-robust-24-ft-age-gender'
processor = Wav2Vec2Processor.from_pretrained(model_name)
model = AgeGenderModel.from_pretrained(model_name)
# Process audio segments to identify unique speakers
def process_func(x: np.ndarray, sampling_rate: int, embeddings: bool = False) -> np.ndarray:
    y = processor(x, sampling_rate=sampling_rate)
    y = y['input_values'][0]
    y = y.reshape(1, -1)
    y = torch.from_numpy(y).to(device)
    with torch.no_grad():
        y = model(y)
        if embeddings:
            y = y[0]
        else:
            y = torch.hstack([y[1], y[2]])
    y = y.detach().cpu().numpy()
    return y
# Initialize speaker dictionary
speakers = {}
for s in speaker_segments:
    start = s.start
    end = s.end
    if end - start < 3000:
        continue
    if s.speaker in speakers:
        continue
    audio = AudioSegment.from_file(FILE_URL)
    sliced_audio = audio[start:end]
    temp_path = 'temp_sliced_audio.mp3'
    sliced_audio.export(temp_path, format="mp3")
    waveform, sample_rate = torchaudio.load(temp_path)
    resampler = Resample(orig_freq=sample_rate, new_freq=16000)
    resampled_waveform = resampler(waveform)
    g = process_func(resampled_waveform, 16000)
    gender = np.argmax(g[0][1:-1])
    gender = 'female' if gender == 0 else 'male'
    speakers[s.speaker] = gender
# Count and print the number of male and female speakers
male_count = sum(1 for gender in speakers.values() if gender == 'male')
female_count = sum(1 for gender in speakers.values() if gender == 'female')
print(f"Number of male speakers: {male_count}")
print(f"Number of female speakers: {female_count}")

if __name__ == "__main__":
    # Run the main functionality
    print("Notebook code extracted successfully")