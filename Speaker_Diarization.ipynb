# ğŸ““ Jupyter Notebook: Speaker_Diarization.ipynb

**Notebook Overview:**
- Total Cells: 28
- Code Cells: 25
- Markdown Cells: 3

**Kernel:** Python 3

**Language:** python Unknown

## ğŸ“‹ Notebook Content:

### ğŸ“ Markdown Cell 1

YTVideo-to-text

---

### ğŸ”¹ Code Cell 2 [Execution: 4]

```python
!pip install -q groq
!pip install -q git+https://github.com/openai/whisper.git
!pip install -q gtts
!pip install -q pytube
```

**Output:**
```
Installing build dependencies ... [?25l[?25hdone
  Getting requirements to build wheel ... [?25l[?25hdone
  Preparing metadata (pyproject.toml) ... [?25l[?25hdone
```

---

### ğŸ”¹ Code Cell 4 [Execution: 5]

```python
import whisper
from pytube import YouTube
import os
from gtts import gTTS
from groq import Groq
```

---

### ğŸ”¹ Code Cell 5 [Execution: 6]

```python
# Download audio from YouTube video
def download_audio_from_youtube(youtube_url, output_file):
    yt = YouTube(youtube_url)
    audio_stream = yt.streams.filter(only_audio=True).first()
    audio_stream.download(filename=output_file)
    return output_file
```

---

### ğŸ”¹ Code Cell 6 [Execution: 7]

```python
# Transcribe audio using Whisper
def transcribe_audio_with_whisper(audio_file):
    model = whisper.load_model("base")
    result = model.transcribe(audio_file)
    return result["text"]
```

---

### ğŸ”¹ Code Cell 7 [Execution: 8]

```python
client = Groq(api_key = "YOUR_API_KEY_HERE")

# Function to get model completion
def get_completion(prompt, model="llama3-70b-8192"):
    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0
    )
    return response.choices[0].message.content
```

---

### ğŸ”¹ Code Cell 8 [Execution: 9]

```python
# Analyze text and create Q&A
def create_qa_from_text(text, model="llama3-70b-8192"):
    prompt = f"""
Create an interesting, informative, and interactive set of 5 textual questions and answers (Q&A) from the content which are relevant to the topic or topics in the text. Generate them as Question 1 and Answer 1 and so on.

Text: {text}
"""
    completion = get_completion(prompt, model)
    return completion
```

---

### ğŸ”¹ Code Cell 9 [Execution: 10]

```python
# Convert Q&A to speech using gTTS
def convert_qa_to_speech(qa_text, output_file):
    tts = gTTS(text=qa_text, lang='en')
    tts.save(output_file)
```

---

### ğŸ”¹ Code Cell 10 [Execution: 11]

```python
# Main execution
youtube_url = "https://www.youtube.com/watch?v=yX5EJf4R77s"
audio_file = "audio.mp3"
download_audio_from_youtube(youtube_url, audio_file)
print(f"Downloaded audio to {audio_file}")

transcript = transcribe_audio_with_whisper(audio_file)
print("Transcription completed")
```

**Output:**
```
Downloaded audio to audio.mp3
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139M/139M [00:01<00:00, 78.4MiB/s]
Transcription completed
```

---

### ğŸ”¹ Code Cell 11 [Execution: 12]

```python
# Save the transcript to a text file
with open("transcript.txt", "w") as file:
    file.write(transcript)
print("Transcript saved as transcript.txt")
```

**Output:**
```
Transcript saved as transcript.txt
```

---

### ğŸ”¹ Code Cell 12 [Execution: 13]

```python
# Create Q&A from the transcript
qa_text = create_qa_from_text(transcript)
```

---

### ğŸ”¹ Code Cell 13 [Execution: 14]

```python
# Save the Q&A to a text file
qa_file = "q_and_a.txt"
with open(qa_file, "w") as file:
    file.write(qa_text)
print("Q&A saved as q_and_a.txt")
```

**Output:**
```
Q&A saved as q_and_a.txt
```

---

### ğŸ”¹ Code Cell 14 [Execution: 15]

```python
# Read the Q&A text from file and remove asterisks
with open(qa_file, "r") as file:
    cleaned_qa_text = file.read().replace('*', '')
```

---

### ğŸ”¹ Code Cell 15 [Execution: 16]

```python
# Save the cleaned Q&A back to the file
with open(qa_file, "w") as file:
    file.write(cleaned_qa_text)
print("Q&A file cleaned from asterisks")
```

**Output:**
```
Q&A file cleaned from asterisks
```

---

### ğŸ”¹ Code Cell 16 [Execution: 17]

```python
# Convert Q&A to speech using gTTS
output_audio_file = "q_and_a_audio.mp3"
convert_qa_to_speech(cleaned_qa_text, output_audio_file)
print(f"Q&A audio saved as {output_audio_file}")
```

**Output:**
```
Q&A audio saved as q_and_a_audio.mp3
```

---

### ğŸ“ Markdown Cell 18

Speaker Classification

---

### ğŸ”¹ Code Cell 19

```python
!pip install -U assemblyai
!pip install -q pytube
!pip install pydub -q
!pip install torch torchaudio transformers
```

**Output:**
```
Collecting assemblyai
  Downloading assemblyai-0.27.0-py3-none-any.whl (69 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m69.1/69.1 kB[0m [31m2.6 MB/s[0m eta [36m0:00:00[0m
[?25hCollecting httpx>=0.19.0 (from assemblyai)
  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m75.6/75.6 kB[0m [31m6.9 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: pydantic!=1.10.7,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from assemblyai) (2.7.4)
Requirement already satisfied: typing-extensions>=3.7 in /usr/local/lib/python3.10/dist-packages (from assemblyai) (4.12.2)
Collecting websockets>=11.0 (from assemblyai)
  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m130.2/130.2 kB[0m [31m17.8 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (3.7.1)
Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (2024.6.2)
Collecting httpcore==1.* (from httpx>=0.19.0->assemblyai)
  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m77.9/77.9 kB[0m [31m13.4 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (3.7)
Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (1.3.1)
Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.19.0->assemblyai)
  Downloading h11-0.14.0-py3-none-any.whl (58 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m58.3/58.3 kB[0m [31m11.6 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.10.7,>=1.7.0->assemblyai) (0.7.0)
Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.10.7,>=1.7.0->assemblyai) (2.18.4)
Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.19.0->assemblyai) (1.2.1)
Installing collected packages: websockets, h11, httpcore, httpx, assemblyai
Successfully installed assemblyai-0.27.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 websockets-12.0
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m57.6/57.6 kB[0m [31m2.4 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)
Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)
Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)
Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)
Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)
Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)
Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)
  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)
Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)
  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)
Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)
  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)
Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)
  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)
Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)
  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)
Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)
  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)
Collecting nvidia-curand-cu12==10.3.2.106 (from torch)
  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)
Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)
  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)
Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)
  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)
Collecting nvidia-nccl-cu12==2.20.5 (from torch)
  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)
Collecting nvidia-nvtx-cu12==12.1.105 (from torch)
  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)
Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)
Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)
  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m21.3/21.3 MB[0m [31m75.1 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)
Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)
Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)
Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)
Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)
Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)
Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)
Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)
Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)
Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12
Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105
```

---

### ğŸ”¹ Code Cell 20

```python
import assemblyai as aai
import numpy as np
import torch
import torch.nn as nn
import torchaudio
from pydub import AudioSegment
from pytube import YouTube
from torchaudio.transforms import Resample
from transformers import Wav2Vec2Processor
from transformers.models.wav2vec2.modeling_wav2vec2 import (
    Wav2Vec2Model,
    Wav2Vec2PreTrainedModel,
)
```

---

### ğŸ”¹ Code Cell 21

```python
# Download audio from YouTube
def download_audio_from_youtube(youtube_url, output_file):
    yt = YouTube(youtube_url)
    audio_stream = yt.streams.filter(only_audio=True).first()
    audio_stream.download(filename=output_file)

link = "https://www.youtube.com/live/8adsVq7Sarw?si=bn98dnLI_fPXwiWq"
download_audio_from_youtube(link, 'cricket_audio.mp3')
```

---

### ğŸ”¹ Code Cell 22

```python
# Transcribe audio and get speaker segments using AssemblyAI
aai.settings.api_key = "YOUR_API_KEY_HERE"
FILE_URL = 'cricket_audio.mp3'
config = aai.TranscriptionConfig(speaker_labels=True)
transcriber = aai.Transcriber()
transcript = transcriber.transcribe(FILE_URL, config=config)
speaker_segments = transcript.utterances
```

---

### ğŸ”¹ Code Cell 23

```python
# Define the model for age and gender classification
class ModelHead(nn.Module):
    def __init__(self, config, num_labels):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.dropout = nn.Dropout(config.final_dropout)
        self.out_proj = nn.Linear(config.hidden_size, num_labels)

    def forward(self, features, **kwargs):
        x = features
        x = self.dropout(x)
        x = self.dense(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x

class AgeGenderModel(Wav2Vec2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.config = config
        self.wav2vec2 = Wav2Vec2Model(config)
        self.age = ModelHead(config, 1)
        self.gender = ModelHead(config, 3)
        self.init_weights()

    def forward(self, input_values):
        outputs = self.wav2vec2(input_values)
        hidden_states = outputs[0]
        hidden_states = torch.mean(hidden_states, dim=1)
        logits_age = self.age(hidden_states)
        logits_gender = torch.softmax(self.gender(hidden_states), dim=1)
        return hidden_states, logits_age, logits_gender

device = 'cpu'
model_name = 'audeering/wav2vec2-large-robust-24-ft-age-gender'
processor = Wav2Vec2Processor.from_pretrained(model_name)
model = AgeGenderModel.from_pretrained(model_name)
```

**Output:**
```
preprocessor_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
config.json:   0%|          | 0.00/2.40k [00:00<?, ?B/s]
vocab.json:   0%|          | 0.00/3.00 [00:00<?, ?B/s]
model.safetensors:   0%|          | 0.00/1.27G [00:00<?, ?B/s]
Some weights of AgeGenderModel were not initialized from the model checkpoint at audeering/wav2vec2-large-robust-24-ft-age-gender and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

---

### ğŸ”¹ Code Cell 24

```python
# Process audio segments to identify unique speakers
def process_func(x: np.ndarray, sampling_rate: int, embeddings: bool = False) -> np.ndarray:
    y = processor(x, sampling_rate=sampling_rate)
    y = y['input_values'][0]
    y = y.reshape(1, -1)
    y = torch.from_numpy(y).to(device)
    with torch.no_grad():
        y = model(y)
        if embeddings:
            y = y[0]
        else:
            y = torch.hstack([y[1], y[2]])
    y = y.detach().cpu().numpy()
    return y

# Initialize speaker dictionary
speakers = {}

for s in speaker_segments:
    start = s.start
    end = s.end
    if end - start < 3000:
        continue
    if s.speaker in speakers:
        continue

    audio = AudioSegment.from_file(FILE_URL)
    sliced_audio = audio[start:end]
    temp_path = 'temp_sliced_audio.mp3'
    sliced_audio.export(temp_path, format="mp3")

    waveform, sample_rate = torchaudio.load(temp_path)
    resampler = Resample(orig_freq=sample_rate, new_freq=16000)
    resampled_waveform = resampler(waveform)
    g = process_func(resampled_waveform, 16000)
    gender = np.argmax(g[0][1:-1])
    gender = 'female' if gender == 0 else 'male'

    speakers[s.speaker] = gender
```

---

### ğŸ”¹ Code Cell 25

```python
# Count and print the number of male and female speakers
male_count = sum(1 for gender in speakers.values() if gender == 'male')
female_count = sum(1 for gender in speakers.values() if gender == 'female')

print(f"Number of male speakers: {male_count}")
print(f"Number of female speakers: {female_count}")
```

**Output:**
```
Number of male speakers: 2
Number of female speakers: 1
```

---
